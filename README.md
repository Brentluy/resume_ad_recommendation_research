# ğŸ“Œ Project Introduction â€” Building an Industrial-Grade CTR System from the AntMRC Dataset

When I started preparing for large-scale recommendation roles (ByteDance Global CRM, Google Personalization, Rokt MLE), one thing became very clear: **every strong candidate needs at least one end-to-end ranking project that truly â€œfeelsâ€ industrial** â€” distributed computing, feature engineering, multi-task modeling, semantic embeddings, and cold-start handling. I didnâ€™t want another toy dataset or a shallow Kaggle notebook. I wanted something that could actually demonstrate my ability to work at production scale.

Thatâ€™s why I chose the **AntMRC dataset** â€” a real-world, multimodal dataset from Ant Financial, containing **mixed structured logs + rich text fields + multi-scene CTR labels**. Even though the raw data reaches the *million/10-million level*, I intentionally restricted myself to **only 10k samples** for the public Colab version. My goal was simple:  
â¡ï¸ *If a system is well-designed, even a 10k subset should reveal the same modeling patterns youâ€™d need at full scale.*

Over the next few weeks, I treated this like a real company project. I wrote the Spark + HDFS ETL pipeline myself, designed the feature store layout, built user/item/UI features, gradually increased model complexity, added semantic towers, and finally solved cold-start with an LLM fallback. I didnâ€™t just â€œstack modelsâ€; every step was motivated by a concrete issue I observed from the data.

The result surprised me even more:  
â¡ï¸ **Baseline DNN AUC: 0.73**  
â¡ï¸ **MMoE multi-task AUC: ~0.80+**  
â¡ï¸ **PLE expert routing AUC: ~0.86â€“0.87**  
â¡ï¸ **Adding BERT-Whitening embeddings: ~0.90+**  
â¡ï¸ **Introducing task-level content towers: ~0.93**  
â¡ï¸ **Final Fusion + LLM cold-start fallback: 0.9739 (Val), 0.9717 (Test)**  
A total lift of **+24 percentage points**, achieved through systematic engineering â€” not guesswork.

Looking back, this project reflects who I am as an engineer:  
- I start from **data reliability** (Spark ETL) before touching any model.  
- I take **feature design seriously** â€” user, item, cross-features, rolling windows, scene-pivot stats.  
- I build models the way real companies do: **MMoE â†’ PLE â†’ fusion gates â†’ semantic towers â†’ LLM fallback**.  
- I debug like someone who has been burned by real systems â€” always watching distribution drift, cold-start performance, input schema stability.  
- And I constantly balance **accuracy vs cost**, because no ranking system runs without constraints.

Even though this public repo only trains on 10k rows, the full pipeline is purposely designed so it can scale to **tens of millions** immediately. The architecture (Spark â†’ ORC â†’ feature store â†’ PyTorch MMoE/PLE â†’ semantic tower â†’ LLM fallback â†’ evaluation suite) mirrors what major tech companies use internally. If given the full AntMRC corpus or an actual production dataset, I am confident this system would continue to push AUC even higher.

More importantly, this project convinced me that I genuinely enjoy this type of work â€” debugging ETL jobs, designing expert networks, understanding scene behavior, improving long-tail recall, and making cold users â€œcome aliveâ€ through text signals. This is exactly the kind of end-to-end ownership expected from ML Engineers in ByteDance Ads, Google Recsys, or Roktâ€™s Ranking team.

This repository is my way of showing that â€” even as a student â€” **I can already think, structure, and deliver like a real recommender-system engineer**.
